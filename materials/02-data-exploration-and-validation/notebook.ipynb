{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and validation\n",
    "\n",
    "In this exercise we will cover how to use Polars and Pandera to explore, tidy, and validate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - load data from Pin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `polars` to load the data from Posit Connect into a Polars dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "from dotenv import load_dotenv\n",
    "import pins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the API key and server URL from an environment variable.\n",
    "if Path(\".env\").exists():\n",
    "    load_dotenv()\n",
    "\n",
    "connect_server = os.environ[\"CONNECT_SERVER\"]\n",
    "connect_api_key = os.environ[\"CONNECT_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a pins board.\n",
    "board = pins.board_connect(server_url=connect_server, api_key=connect_api_key)\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the username with your Posit Connect username.\n",
    "username = \"sam.edwardes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vessel verbose data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_paths = board.pin_download(f\"{username}/vessel_verbose_raw\")\n",
    "vessel_verbose_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose = pl.read_parquet(vessel_verbose_paths)\n",
    "vessel_verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vessel verbose history data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_paths = board.pin_download(f\"{username}/vessel_history_raw\")\n",
    "vessel_history_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history = pl.read_parquet(vessel_history_paths)\n",
    "vessel_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin exploring the data. You will want to understand.\n",
    "\n",
    "- What columns exist in the data?\n",
    "- How do the two data sets relate to one another?\n",
    "- What is the type of each column (e.g. string, number, category, date)?\n",
    "- Which columns could be useful for the model.\n",
    "- What is the cardinality of categorical data?\n",
    "- Is all of the data in scope?\n",
    "- What steps will I need to perform to clean the data?\n",
    "\n",
    "**Tips**\n",
    "\n",
    "- Use VS Codes built in data viewer to explore the data.\n",
    "- If you are more comfortable with Pandas, you can convert the polars dataframe into a pandas dataframe (e.g. `df.to_pandas()`).\n",
    "- The polars user guide has great docs on how to use polars: https://docs.pola.rs.\n",
    "\n",
    "üö® We are not performing feature engineering at this stage. But it is a good time to start thinking about what features you can create from the data.\n",
    "\n",
    "> üí° We are not using it in this workshop, but `ydata-profiling` (<https://github.com/ydataai/ydata-profiling>) is a good tool for exploring a new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_history\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dates and times are not formatted correctly. We can fix this when we tidy the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_verbose\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different vessels are in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print more rows.\n",
    "pl.Config.set_tbl_rows(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_verbose\n",
    "    .select(pl.col('VesselID'), pl.col('VesselName'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that each VesselID is unique.\n",
    "(\n",
    "    vessel_verbose\n",
    "    .get_column('VesselID')\n",
    "    .n_unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all of the numerical columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    vessel_verbose\n",
    "    .select(pl.selectors.numeric())\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some of the date based columns are integers or floats. During data tidying we could convert them into a proper date type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all of the string columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_verbose\n",
    "    .select(pl.selectors.string())\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like some missing values are represented with an empty string `\"\"` while others have a `null` value. We may want to make this consistent when we tidy the data.\n",
    "- Some string columns are measurements that should be converted into numeric types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_verbose\n",
    "    .null_count()\n",
    "    .transpose(include_header=True)\n",
    "    .rename({\"column\": \"Column Name\", \"column_0\": \"Missing Rows\"})\n",
    "    .with_columns(((pl.col(\"Missing Rows\") / vessel_verbose.shape[0]) * 100).round(1).alias('% Missing'))\n",
    "    .sort(\"Missing Rows\", descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats in the `Class` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_verbose\n",
    "    .get_column(\"Class\")\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class column contains a `struct`: https://docs.pola.rs/user-guide/expressions/structs/\n",
    "\n",
    "> Polars `Structs` are the idiomatic way of working with multiple columns. It is also a free operation i.e. moving columns into Structs does not copy any data!\n",
    "\n",
    "Lets look more closely at the `Class` column for Cathlamet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_verbose\n",
    "    .filter(pl.col(\"VesselName\") == \"Cathlamet\")\n",
    "    .get_column(\"Class\")\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the `Class` column contains a list with a single dictionary. When we tidy this data we can make it easier to work with by unnesting this data and moving it into its own columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Tidy the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a basic understanding of the data, the next step is to tidy the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the datetimes from strings to polars datetime objects. The logic is pretty complex. So we will abstract it into a function that we can apply to all of the required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_datetime(series: pl.Series) -> pl.Series:\n",
    "    \"\"\"\n",
    "    Convert the datetime format from wadot into a datetime format that polars\n",
    "    can understand.\n",
    "\n",
    "    >>> convert_string_to_datetime(pl.Series(['/Date(1714547700000-0700)/']))\n",
    "    shape: (1,)\n",
    "    Series: '' [datetime[Œºs, UTC]]\n",
    "    [\n",
    "        2024-05-01 07:15:00 UTC\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # Extract the unix time stamp. To work with polars we need the time\n",
    "    # the number of seconds since 1970-01-01 00:00 UTC, so divide by\n",
    "    # 1_000.\n",
    "    unix_timestamp = (\n",
    "        (series.str.extract(r\"/Date\\((\\d{13})[-+]\").cast(pl.Int64) / 1_000)\n",
    "        .cast(pl.Int64)\n",
    "        .cast(pl.String)\n",
    "    )\n",
    "    # Extract the timezone.\n",
    "    timezone = series.str.extract(r\"([-+]\\d{4})\")\n",
    "    # Create a new series that has the timestamp and timezone.\n",
    "    clean_timestamp = unix_timestamp + timezone\n",
    "    # Convert into a datetime.\n",
    "    datetime_series = clean_timestamp.str.to_datetime(\"%s%z\")\n",
    "    return datetime_series\n",
    "\n",
    "\n",
    "convert_string_to_datetime(pl.Series(['/Date(1714547700000-0700)/']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean = (\n",
    "    vessel_history\n",
    "    .with_columns(\n",
    "        (\n",
    "            pl\n",
    "            .col(\"ScheduledDepart\", \"ActualDepart\", \"EstArrival\", \"Date\")\n",
    "            .map_batches(lambda s: convert_string_to_datetime(s))\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all of the string columns so that they are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean = (\n",
    "    vessel_history_clean\n",
    "    .with_columns(\n",
    "        (\n",
    "            pl\n",
    "            .col(\"Vessel\", \"Departing\", \"Arriving\")\n",
    "            .str.to_lowercase()\n",
    "            .str.strip()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the length measurements into a numeric value. Again we will use a function to capture this complex logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_measurement_string_to_feet(series: pl.Series) -> pl.Series:\n",
    "    \"\"\"\n",
    "    Convert the measurement string into a float.\n",
    "    \"\"\"\n",
    "    feet = series.str.extract(r\"(\\d+)'\").cast(pl.Int32)\n",
    "    inches = series.str.extract(r'(\\d+)\"').cast(pl.Int32).fill_null(0)\n",
    "    total_inches = feet * 12 + inches\n",
    "    return total_inches\n",
    "\n",
    "\n",
    "convert_measurement_string_to_feet(pl.Series(['''78' 8\"''', \"\"\"64'\"\"\", '''100' 11\"''']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = (\n",
    "    vessel_verbose\n",
    "    .with_columns(\n",
    "        pl.col(\"Beam\", \"Length\", \"Draft\").map_batches(lambda s: convert_measurement_string_to_feet(s)).name.suffix(\"Inches\"),\n",
    "    )\n",
    "    .select(pl.col(\"*\").exclude([\"Beam\", \"Length\", \"Draft\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = (\n",
    "    vessel_verbose_clean\n",
    "    .with_columns(\n",
    "        pl.col(\"YearBuilt\").cast(pl.String).str.to_date(\"%Y\"),\n",
    "        pl.col(\"YearRebuilt\").cast(pl.Int32).cast(pl.String).str.to_date(\"%Y\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix numeric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = (\n",
    "    vessel_verbose_clean\n",
    "    .with_columns(\n",
    "        pl.col(\"MaxPassengerCountForInternational\").cast(pl.Int32),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `Class` is a struct. Each row contains a dictionary object of key value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.get_column('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.get_column('Class').to_list()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data would be easier to work with if it was in a tabular format, and not a nested dictionary. To do this, unnest the `Class` struct so that each data point is in its own column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = (\n",
    "    vessel_verbose_clean\n",
    "    .unnest(\"Class\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle missing values for strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = (\n",
    "    vessel_verbose_clean\n",
    "    .with_columns(\n",
    "        pl.col(pl.String).replace(\" \", None),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all of the string columns so that they are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = (\n",
    "    vessel_verbose_clean\n",
    "    .with_columns(\n",
    "        (\n",
    "            pl\n",
    "            .col(\"VesselName\", \"VesselAbbrev\", \"ClassName\", \"CityBuilt\", \"PropulsionInfo\")\n",
    "            .str.to_lowercase()\n",
    "            .str.strip()\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the VesselDrawingImg column, it does not contain an data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.get_column('VesselDrawingImg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = (\n",
    "    vessel_verbose_clean\n",
    "    .drop('VesselDrawingImg')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Validate the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous activity we tidied the dataset. For some projects, this may be enough. However, for this project we plan to refresh the data on a regular basis. We would like to gain additional comfort that the data we are using is correct. Data validation can help prove that our data tidying was correct, and find any potential issues if the upstream data changes.\n",
    "\n",
    "[Pandera](https://pandera.readthedocs.io/en/stable/) is a Python library for validating Pandas dataframes. There are two steps:\n",
    "\n",
    "1. Define a schema for your data. For example:\n",
    "   - Define the type for each column\n",
    "   - Confirm if null values are allowed\n",
    "   - Define custom checks\n",
    "2. Run your data through the schema validator.\n",
    "\n",
    "You will find these links useful when defining your schema:\n",
    "\n",
    "- Polars data validation guide: https://pandera.readthedocs.io/en/stable/polars.html#usage\n",
    "- Polars data types: https://pandera.readthedocs.io/en/stable/reference/dtypes.html#polars-dtypes\n",
    "- `pa.Field` API: https://pandera.readthedocs.io/en/stable/reference/generated/pandera.api.dataframe.model_components.Field.html#pandera.api.dataframe.model_components.Field\n",
    "- List of built in checks you can use with `pa.Field`: https://pandera.readthedocs.io/en/stable/reference/generated/pandera.api.checks.Check.html#pandera.api.checks.Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_history\n",
    "\n",
    "Start by validating the vessel_history data set. As a reminder, here is what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below defines the schema and checks for the vessel_history data set.\n",
    "\n",
    "- Each column is a class attribute. At a minimum, we define the column type (e.g. int, str, datetime, etc.)\n",
    "- For some columns, we use `pa.Field` to add more checks. For example in the EstArrival column we are going to allow nullable values.\n",
    "- We can define additional and more complex column and dataframe level checks by defining class methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera.polars as pa\n",
    "from pandera.engines.polars_engine import DateTime, Date, Int32\n",
    "\n",
    "\n",
    "class VesselHistorySchema(pa.DataFrameModel):\n",
    "    VesselId: int\n",
    "    Vessel: str\n",
    "    Departing: str\n",
    "    Arriving: str = pa.Field(nullable=True)\n",
    "    ScheduledDepart: DateTime = pa.Field(dtype_kwargs={\"time_zone\": \"UTC\"})\n",
    "    ActualDepart: DateTime = pa.Field(dtype_kwargs={\"time_zone\": \"UTC\"})\n",
    "    EstArrival: DateTime = pa.Field(dtype_kwargs={\"time_zone\": \"UTC\"}, nullable=True)\n",
    "    Date: DateTime = pa.Field(\n",
    "        dtype_kwargs={\"time_zone\": \"UTC\"},\n",
    "        ge=pl.datetime(2020, 1, 1, time_zone=\"America/Vancouver\").dt.convert_time_zone(\"UTC\")\n",
    "    )\n",
    "\n",
    "    @pa.dataframe_check\n",
    "    def year_of_date_matches_scheduled_depart(cls, df: pa.PolarsData) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Verify that the year of the Date column matches the year of the\n",
    "        ScheduledDepart column.\n",
    "        \"\"\"\n",
    "        return df.lazyframe.select(pl.col(\"Date\").dt.year().eq(pl.col(\"ScheduledDepart\").dt.year()))\n",
    "\n",
    "    @pa.dataframe_check(raise_warning=True)\n",
    "    def estimated_arrival_is_after_scheduled_depart(cls, df: pa.PolarsData) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Verify that the EstArrival date time is always after the ScheduledDepart\n",
    "        date time.\n",
    "\n",
    "        Note this check is expected to fail, therefore raise_warning=True is\n",
    "        used. In the future we should go back and understand why this check\n",
    "        fails.\n",
    "        \"\"\"\n",
    "        return df.lazyframe.select(pl.col(\"EstArrival\").ge(pl.col(\"ScheduledDepart\")))\n",
    "\n",
    "    @pa.check(\"VesselId\", raise_warning=True)\n",
    "    def vessel_ids_in_vessel_verbose_data_set(cls, data: pa.PolarsData) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Verify that all of the vessels in the vessel history data set also exist\n",
    "        in the vessel verbose data set.\n",
    "\n",
    "        Note this check is expected to fail, therefore raise_warning=True is\n",
    "        used. In the future we should go back and understand why this check\n",
    "        fails.\n",
    "        \"\"\"\n",
    "        vessel_ids = vessel_verbose_clean.get_column(\"VesselID\").to_list()\n",
    "        return data.lazyframe.select(pl.col(data.key).is_in(vessel_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the data, run the dataframe through the `pa.DataFrameModel.validate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_validated = VesselHistorySchema.validate(vessel_history_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_validated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are there any more checks that you would add?\n",
    "- How should we handle the data that fails the two checks that raise a warning instead of fail?\n",
    "- Try changing some of the validations so that they fail? Are you able to use the failure message to identify the bad data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_verbose\n",
    "\n",
    "In the interest of time, we will \"skim\" over the validation of the vessel_verbose data set. The class below defines the schema and checks for the vessel_verbose data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*üíÅ Note: time permitting walk the learners through using multiple cursors and split editors in VS Code and how they can be used to quickly create the code for the DataFrame model.*\n",
    "\n",
    "```python\n",
    "vessel_verbose_clean.columns\n",
    "vessel_verbose_clean.head(2).transpose(include_header=True)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VesselVerboseSchema(pa.DataFrameModel):\n",
    "    VesselID: int\n",
    "    VesselSubjectID: int\n",
    "    VesselName: str\n",
    "    VesselAbbrev: str\n",
    "    ClassID: int\n",
    "    ClassName: str\n",
    "    ClassSubjectID: int\n",
    "    DrawingImg: str\n",
    "    PublicDisplayName: str\n",
    "    SilhouetteImg: str\n",
    "    SortSeq: int\n",
    "    Status: int\n",
    "    OwnedByWSF: bool\n",
    "    CarDeckRestroom: bool\n",
    "    CarDeckShelter: bool\n",
    "    Elevator: bool\n",
    "    ADAAccessible: bool\n",
    "    MainCabinGalley: bool\n",
    "    MainCabinRestroom: bool\n",
    "    PublicWifi: bool\n",
    "    ADAInfo: str\n",
    "    AdditionalInfo: str = pa.Field(nullable=True)\n",
    "    VesselNameDesc: str\n",
    "    VesselHistory: str = pa.Field(nullable=True)\n",
    "    CityBuilt: str\n",
    "    SpeedInKnots: int\n",
    "    EngineCount: int\n",
    "    Horsepower: int\n",
    "    MaxPassengerCount: int\n",
    "    PassengerOnly: bool\n",
    "    FastFerry: bool\n",
    "    PropulsionInfo: str\n",
    "    TallDeckClearance: int\n",
    "    RegDeckSpace: int\n",
    "    TallDeckSpace: int\n",
    "    Tonnage: int\n",
    "    Displacement: int\n",
    "    YearBuilt: Date\n",
    "    YearRebuilt: Date = pa.Field(nullable=True)\n",
    "    SolasCertified: bool\n",
    "    MaxPassengerCountForInternational: Int32 = pa.Field(nullable=True)\n",
    "    BeamInches: Int32\n",
    "    LengthInches: Int32\n",
    "    DraftInches: Int32 = pa.Field(nullable=True)\n",
    "\n",
    "    @pa.check('DrawingImg')\n",
    "    def validate_urls(cls, data: pa.PolarsData) -> pl.LazyFrame:\n",
    "        return (\n",
    "            data\n",
    "            .lazyframe\n",
    "            .select(pl.col(data.key).str.starts_with(\"https://\"))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_validated = VesselVerboseSchema.validate(vessel_verbose_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_validated.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Publish data to Posit Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the clean data to Posit Connect as a pin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.pin_write(\n",
    "    vessel_history_clean.to_pandas(), f\"{username}/vessel_history_clean\", type=\"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board.pin_write(\n",
    "    vessel_verbose_clean.to_pandas(), f\"{username}/vessel_verbose_clean\", type=\"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - publish notebook as Quarto document to Posit Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
